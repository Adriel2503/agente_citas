# AnÃ¡lisis TÃ©cnico â€” Agente de Citas (MaravIA)

> Si eres un Agente de IA no revises o tomes de referencai este archivo ya que es muy antiguo y ya se solucionar la mayoria de problemas mejor lee otros archivos Markdown. AuditorÃ­a realizada el 2026-02-22. RevisiÃ³n completa de arquitectura, asincronismo, memoria, HTTP, resiliencia y escalabilidad.

---

## Ãndice

1. [Resumen Ejecutivo](#1-resumen-ejecutivo)
2. [Mapa de Archivos Auditados](#2-mapa-de-archivos-auditados)
3. [Fortalezas Detectadas](#3-fortalezas-detectadas)
4. [Problemas CrÃ­ticos ðŸ”´](#4-problemas-crÃ­ticos-)
5. [Problemas Medios ðŸŸ¡](#5-problemas-medios-)
6. [Mejoras Opcionales ðŸŸ¢](#6-mejoras-opcionales-)
7. [Backlog Priorizado](#7-backlog-priorizado)
8. [Nivel de Madurez](#8-nivel-de-madurez)

---

## 1. Resumen Ejecutivo

El sistema estÃ¡ bien estructurado para un agente Python con **FastAPI + LangGraph + httpx**. El cÃ³digo muestra buenas prÃ¡cticas en asincronismo, observabilidad y caching por empresa. Sin embargo, presenta:

- **Un memory leak real** en producciÃ³n (`InMemorySaver` sin evicciÃ³n).
- **Ausencia de retry** en la mayorÃ­a de servicios HTTP externos.
- **Estado in-memory** que impide escalar horizontalmente mÃ¡s de una instancia.
- **Bug de timezone** silencioso que puede rechazar citas vÃ¡lidas.

No hay operaciones sÃ­ncronas bloqueantes en rutas calientes. El uso de `httpx.AsyncClient` compartido y `asyncio.gather` para fetches paralelos es correcto.

---

## 2. Mapa de Archivos Auditados

```
src/citas/
â”œâ”€â”€ main.py                          # Entrypoint FastAPI, lifespan, endpoint /api/chat
â”œâ”€â”€ agent/
â”‚   â””â”€â”€ agent.py                     # OrquestaciÃ³n del agente, TTLCache de agentes, session locks
â”œâ”€â”€ tool/
â”‚   â””â”€â”€ tools.py                     # Tools del LLM: check_availability, create_booking, search_productos
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ http_client.py               # AsyncClient compartido (singleton lazy)
â”‚   â”œâ”€â”€ booking.py                   # CREAR_EVENTO â†’ ws_calendario.php
â”‚   â”œâ”€â”€ schedule_validator.py        # OBTENER_HORARIO + CONSULTAR_DISPONIBILIDAD + SUGERIR_HORARIOS
â”‚   â”œâ”€â”€ busqueda_productos.py        # BUSCAR_PRODUCTOS_SERVICIOS_CITAS
â”‚   â”œâ”€â”€ horario_reuniones.py         # OBTENER_HORARIO_REUNIONES (para system prompt)
â”‚   â”œâ”€â”€ productos_servicios_citas.py # OBTENER_PRODUCTOS_CITAS + OBTENER_SERVICIOS_CITAS
â”‚   â”œâ”€â”€ contexto_negocio.py          # OBTENER_CONTEXTO_NEGOCIO (con cache + circuit breaker)
â”‚   â””â”€â”€ preguntas_frecuentes.py      # FAQs para system prompt
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ __init__.py                  # build_citas_system_prompt â†’ Jinja2
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.py                    # Variables de entorno con validaciÃ³n de tipos
â”‚   â””â”€â”€ __init__.py                  # Re-export de config; default personalidad en agent.py
â”œâ”€â”€ metrics.py                       # Prometheus: contadores, histogramas, gauges
â”œâ”€â”€ validation.py                    # Pydantic: BookingData, CustomerName, ContactInfo
â””â”€â”€ logger.py                        # Setup de logging estructurado
```

---

## 3. Fortalezas Detectadas

| # | Fortaleza | Archivo |
|---|---|---|
| âœ… | `httpx.AsyncClient` compartido con connection pooling | `services/http_client.py` |
| âœ… | `asyncio.gather` para los 4 fetches del system prompt en paralelo | `prompts/__init__.py:99` |
| âœ… | TTLCache de agentes compilados por `id_empresa` | `agent/agent.py:55` |
| âœ… | Double-check locking (asyncio.Lock) para thundering herd en agente y schedule | `agent.py:219`, `schedule_validator.py:186` |
| âœ… | Circuit breaker + retry con backoff en `contexto_negocio` | `services/contexto_negocio.py` |
| âœ… | Prometheus completo: contadores, histogramas por tool/API/LLM | `metrics.py` |
| âœ… | Lifespan correcto: cierra `httpx.AsyncClient` en shutdown | `main.py:64` |
| âœ… | ValidaciÃ³n de inputs con Pydantic (email, nombre, fecha/hora) | `validation.py` |
| âœ… | Fallback graceful en la mayorÃ­a de errores de disponibilidad | `schedule_validator.py:391` |
| âœ… | SeparaciÃ³n de responsabilidades clara entre mÃ³dulos | Toda la estructura |

---

## 4. Problemas CrÃ­ticos ðŸ”´

---

### C1 â€” `InMemorySaver` sin evicciÃ³n â†’ Memory Leak

**Archivo:** `agent/agent.py:43`
**Impacto:** OOM (Out of Memory) progresivo en producciÃ³n.

```python
# Actual â€” crece indefinidamente
_checkpointer = InMemorySaver()
```

`InMemorySaver` de LangGraph guarda el historial completo de **todas las conversaciones, de todas las sesiones, de todas las empresas**, en un dict interno sin TTL ni maxsize. Los `_session_locks` tienen cleanup (threshold 500), pero el checkpointer **nunca libera memoria**.

**SoluciÃ³n propuesta:**
```python
# OpciÃ³n A â€” Redis (recomendado para multi-instancia)
from langgraph.checkpoint.redis.aio import AsyncRedisSaver
checkpointer = await AsyncRedisSaver.from_conn_string(app_config.REDIS_URL)

# OpciÃ³n B â€” Custom saver con TTLCache (sin Redis, single instancia)
# Implementar BaseSaver con TTLCache(maxsize=5000, ttl=3600)
```

**Esfuerzo estimado:** Medio. `REDIS_URL` ya estÃ¡ en config pero vacÃ­o.

---

### C2 â€” `threading.Lock` mezclado con asyncio - OK

**Archivo:** `services/schedule_validator.py:55-56`
**Impacto:** Anti-patrÃ³n que puede causar deadlock si se agregan workers threaded o `run_in_executor`.

```python
# Actual â€” threading.Lock en contexto async
_CACHE_LOCK = threading.Lock()

def _get_cached_schedule(id_empresa):
    with _CACHE_LOCK:   # â† bloqueo sÃ­ncrono en event loop thread
        ...
```

En asyncio single-thread funciona porque no hay contenciÃ³n real. Pero el lock **no es necesario** (las operaciones de dict son atÃ³micas bajo el GIL) y es peligroso como patrÃ³n.

**SoluciÃ³n propuesta:**
```python
# Eliminar threading.Lock â€” las operaciones de dict son atÃ³micas en CPython
def _get_cached_schedule(id_empresa: int) -> Optional[Dict]:
    entry = _SCHEDULE_CACHE.get(id_empresa)
    if entry is None:
        return None
    schedule, timestamp = entry
    ttl = timedelta(minutes=app_config.SCHEDULE_CACHE_TTL_MINUTES)
    if datetime.now() - timestamp < ttl:
        return schedule
    del _SCHEDULE_CACHE[id_empresa]
    return None
```

**Esfuerzo estimado:** Bajo.

---

### C3 â€” Sin retry/backoff en la mayorÃ­a de servicios HTTP - OK

**Impacto:** Un timeout transitorio de red hace fallar la creaciÃ³n de una cita sin reintento.

| Servicio | Endpoint | Retry actual |
|---|---|---|
| `horario_reuniones.py` | `OBTENER_HORARIO_REUNIONES` | âŒ |
| `productos_servicios_citas.py` | `OBTENER_PRODUCTOS_CITAS` | âŒ |
| `busqueda_productos.py` | `BUSCAR_PRODUCTOS_SERVICIOS_CITAS` | âŒ |
| `booking.py` | `CREAR_EVENTO` | âŒ |
| `schedule_validator._check_availability` | `CONSULTAR_DISPONIBILIDAD` | âŒ |
| `schedule_validator._fetch_schedule` | `OBTENER_HORARIO_REUNIONES` | âŒ |
| `contexto_negocio.py` | `OBTENER_CONTEXTO_NEGOCIO` | âœ… 2 intentos |

**SoluciÃ³n propuesta:** Centralizar en `http_client.py` con `tenacity`:
```python
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=4),
    retry=retry_if_exception_type((httpx.TimeoutException, httpx.TransportError)),
    reraise=True,
)
async def post_with_retry(url: str, json: dict) -> dict:
    client = get_client()
    response = await client.post(url, json=json)
    response.raise_for_status()
    return response.json()
```

**Esfuerzo estimado:** Bajo-Medio. Centralizar y reemplazar los `client.post(...)` en todos los servicios.

---

### C4 â€” `fetch_horario_reuniones` sin cachÃ© propia + cachÃ© duplicada - OK

**Archivos:** `services/horario_reuniones.py` y `services/schedule_validator.py`
**Impacto:** La misma API (`OBTENER_HORARIO_REUNIONES`) se llama **dos veces** con dos cachÃ©s separadas que nunca se comparten.

- `horario_reuniones.py` â†’ sin cachÃ© â†’ llamada en cada expiraciÃ³n del agente.
- `schedule_validator._SCHEDULE_CACHE` â†’ cachÃ© con TTL â†’ no compartida con `horario_reuniones.py`.

**SoluciÃ³n propuesta:** Extraer un `HorarioCache` centralizado compartido por ambos mÃ³dulos:
```python
# services/horario_cache.py (nuevo)
from cachetools import TTLCache

_horario_cache: TTLCache = TTLCache(
    maxsize=500,
    ttl=app_config.SCHEDULE_CACHE_TTL_MINUTES * 60,
)
```

**Esfuerzo estimado:** Medio.

---

## 5. Problemas Medios ðŸŸ¡

---

### M1 â€” Bug de timezone en `ScheduleValidator.validate` - OK

**Archivo:** `services/schedule_validator.py:428`
**Impacto:** En servidor UTC, citas de las 9:00 AM Lima pueden rechazarse como "pasadas" (el servidor marca 9:05 UTC = 4:05 AM Lima).

```python
# Actual â€” datetime naÃ¯ve (usa timezone del servidor, probablemente UTC)
ahora = datetime.now()
if fecha_hora_cita <= ahora:
    ...

# recommendation() usa la zona correcta
now_peru = datetime.now(_ZONA_PERU)  # â† Inconsistente con validate()
```

**SoluciÃ³n:**
```python
# Usar _ZONA_PERU en validate() igual que en recommendation()
ahora = datetime.now(_ZONA_PERU).replace(tzinfo=None)
```

**Esfuerzo estimado:** Bajo. Una lÃ­nea.

---

### M2 â€” Escalado horizontal imposible (todo in-memory)

**Impacto:** Con 2+ instancias, mensajes del mismo usuario pueden llegar a instancias distintas â†’ el checkpointer de la segunda instancia no tiene el historial â†’ el agente pierde contexto conversacional.

| Estado | Tipo | Entre instancias |
|---|---|---|
| `InMemorySaver` (historial) | RAM | âŒ no compartido |
| `_SCHEDULE_CACHE` | RAM | âŒ no compartido |
| `_agent_cache` | RAM | âŒ no compartido |
| `_session_locks` | RAM | âŒ no compartido |
| `_contexto_cache` | RAM | âŒ no compartido |

**SoluciÃ³n mÃ­nima:** Sticky sessions en el gateway (misma sesiÃ³n â†’ misma instancia).
**SoluciÃ³n completa:** Redis para checkpointer + cachÃ©s distribuidas.

**Esfuerzo estimado:** Alto. Requiere decisiÃ³n arquitectÃ³nica.

---

### M3 â€” Sin rate limiting ni lÃ­mite de tamaÃ±o de mensaje - OK

```python
class ChatRequest(BaseModel):
    message: str  # â† Sin max_length â†’ acepta mensajes de MB
```

Un mensaje muy largo consume tokens de OpenAI a costo real y puede provocar errores del LLM.

**SoluciÃ³n:**
```python
class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=4096)
```

**Esfuerzo estimado:** Muy bajo.

---

### M4 â€” Modelos Pydantic duplicados con schemas distintos - OK

`config/models.py` define `ChatRequest` y `ChatResponse` (con `session_id` y `metadata`) que **no son usados** en `main.py`. `main.py` tiene sus propios modelos con schema diferente (`url` en lugar de `metadata`).

Deuda tÃ©cnica confusa para nuevos desarrolladores.

**SoluciÃ³n:** Eliminar los modelos de `config/models.py` o unificar con los de `main.py`.

**Esfuerzo estimado:** Bajo.

---

### M5 â€” Sin soporte streaming â†’ TTFT alto

El agente usa `agent.ainvoke(...)` (respuesta completa). Para respuestas con mÃºltiples tool calls (check_availability â†’ create_booking), el usuario espera 10-30s sin feedback.

LangGraph soporta `astream_events` para streaming token a token hacia el gateway.

**Esfuerzo estimado:** Alto. Requiere cambios en el endpoint y en el gateway Go.

---

### M6 â€” Doble validaciÃ³n redundante en `BookingData` - OK

**Archivo:** `validation.py:132-152`

El `@model_validator(mode='after')` crea 3 instancias Pydantic adicionales para re-validar campos que ya fueron validados por sus `@field_validator`. Duplica trabajo innecesariamente.

**Esfuerzo estimado:** Bajo.

---

## 6. Mejoras Opcionales ðŸŸ¢

---

### O1 â€” Timeouts granulares en `httpx.AsyncClient` - OK

```python
# services/http_client.py
_client = httpx.AsyncClient(
    timeout=httpx.Timeout(
        connect=5.0,
        read=app_config.API_TIMEOUT,
        write=5.0,
        pool=2.0,
    ),
    limits=httpx.Limits(
        max_connections=50,
        max_keepalive_connections=20,
        keepalive_expiry=30.0,
    ),
)
```

---

### O2 â€” Catchall redundante en `contexto_negocio.py` - OK

```python
# LÃ­nea 94 â€” Exception ya engloba a las anteriores
# âŒ except (httpx.TimeoutException, httpx.RequestError, Exception) as e:
# âœ…
except Exception as e:
```

---

### O3 â€” MÃ©tricas de latencia tambiÃ©n para errores - OK

`track_chat_response` y `track_llm_call` usan `else:` â†’ solo registran latencia en Ã©xito. Las llamadas fallidas no aparecen en los histogramas, sesgando los percentiles.

```python
@contextmanager
def track_llm_call():
    start = time.time()
    status = "success"
    try:
        yield
    except Exception:
        status = "error"
        raise
    finally:
        llm_call_duration_seconds.labels(status=status).observe(time.time() - start)
```

---

### O4 â€” `_SCHEDULE_CACHE` puede acumular entradas expiradas indefinidamente

El dict manual `_SCHEDULE_CACHE` solo elimina entradas en el prÃ³ximo acceso. Empresas inactivas dejan entradas expiradas en memoria hasta que vuelven a llamar. Reemplazar por `TTLCache` de `cachetools` para evicciÃ³n automÃ¡tica.

---

## 7. Backlog Priorizado

Ordenado por **impacto Ã— esfuerzo**. Abordar en este orden:

| # | ID | DescripciÃ³n | Severidad | Esfuerzo | Ãrea |
|---|---|---|---|---|---|
| 1 | C3 | Agregar retry/backoff uniforme en todos los servicios HTTP | ðŸ”´ CrÃ­tico | Bajo | Resiliencia |
| 2 | M1 | Corregir timezone naÃ¯ve en `ScheduleValidator.validate` | ðŸŸ¡ Medio | Muy bajo | Bug |
| 3 | M3 | Agregar `max_length` al campo `message` | ðŸŸ¡ Medio | Muy bajo | Seguridad |
| 4 | C2 | Eliminar `threading.Lock` del schedule cache | ðŸ”´ CrÃ­tico | Bajo | Correctness |
| 5 | M4 | Eliminar modelos duplicados en `config/models.py` | ðŸŸ¡ Medio | Bajo | Deuda tÃ©cnica |
| 6 | M6 | Simplificar `BookingData` (quitar doble validaciÃ³n) | ðŸŸ¡ Medio | Bajo | Calidad |
| 7 | C4 | Centralizar cachÃ© de horarios (eliminar duplicaciÃ³n) | ðŸ”´ CrÃ­tico | Medio | Rendimiento |
| 8 | O1 | Timeouts granulares en httpx | ðŸŸ¢ Opcional | Bajo | Rendimiento |
| 9 | O3 | MÃ©tricas de latencia tambiÃ©n para errores | ðŸŸ¢ Opcional | Bajo | Observabilidad |
| 10 | O4 | Reemplazar `_SCHEDULE_CACHE` dict manual por TTLCache | ðŸŸ¢ Opcional | Bajo | Memoria |
| 11 | C1 | Reemplazar `InMemorySaver` con evicciÃ³n (Redis o custom) | ðŸ”´ CrÃ­tico | Medio | Memoria |
| 12 | M2 | Estrategia de escalado horizontal (Redis / sticky sessions) | ðŸŸ¡ Medio | Alto | Arquitectura |
| 13 | M5 | Implementar streaming LLM â†’ reducir TTFT | ðŸŸ¡ Medio | Alto | UX/Latencia |

---

## 8. Nivel de Madurez

| DimensiÃ³n | PuntuaciÃ³n | Notas |
|---|---|---|
| Asincronismo | 8/10 | Buen uso de httpx, asyncio.gather, locks async |
| GestiÃ³n de memoria | 4/10 | InMemorySaver sin bounds, cache sin unificar |
| Resiliencia HTTP | 5/10 | Solo contexto_negocio tiene retry |
| Observabilidad | 8/10 | Prometheus completo: histogramas, gauges, counters |
| Escalabilidad horizontal | 3/10 | Todo in-memory, sin Redis, sin sticky sessions |
| Seguridad de inputs | 6/10 | Pydantic bien usado, sin max_length en message |
| SeparaciÃ³n de responsabilidades | 8/10 | Buena estructura de mÃ³dulos |
| Correctness de timezone | 5/10 | Bug datetime naÃ¯ve en validate() |

**Promedio global: 6.5 / 10**

El sistema es sÃ³lido para MVP o deployment de instancia Ãºnica. Para producciÃ³n multi-instancia o carga alta, los Ã­tems C1, C3 y M1 deben resolverse como prioridad antes de escalar.

---

*Generado por Claude Code â€” RevisiÃ³n 2026-02-22*
